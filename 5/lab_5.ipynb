{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание на пятую лабораторную:\n",
    "Реализовать вопросно ответную систему типа \"retriever-reader\"\n",
    "Напоминаю идею: ридер получает на вход текст и вопрос к нему, и возвращает место в тексте, которое является ответом на вопрос. Ретривер ищет из списка текстов тот, в котором с наибольшей вероятностью найдется ответ на вопрос (по косинусной мере между вопросом и каждым текстом) -- вот это нужно реализовать. Вектора текстов для ретривера можете генерировать, как вам больше нравится -- трансформеры/word2vec/tf-idf или что-то еще\n",
    "На выходе должно быть приблизительно следующее:\n",
    "1. Функция-ретривер: получает на вход тексты и вопрос, возвращает нужный текст\n",
    "2. Функция-ридер: получает текст и вопрос, возвращает ответ\n",
    "3. Функция, внутри которой последовательно вызывается ретривер и ридер\n",
    "4. Добавьте, пожалуйста, несколько примеров работы вашего кода, чтобы было видно, что вы передали тексты и вопрос, и получили правильный ответ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from typing import List, Union\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция-ретривер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    def __init__(self, MODEL):\n",
    "        global device\n",
    "        self.model_ = BertModel.from_pretrained(MODEL, output_hidden_states=True)\n",
    "        self.model_.eval()  # Устанавливаем модель в режим оценки\n",
    "        self.model_.to(device)  # Перемещаем модель на указанное устройство\n",
    "        self.tokenizer_ = BertTokenizer.from_pretrained(MODEL, do_lower_case=True)  # Инициализация токенизатора\n",
    "    \n",
    "    # Преобразование текста в векторное представление\n",
    "    def vectors_from_texts(self, text):\n",
    "        marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "        tokenized_text = self.tokenizer_.tokenize(marked_text)\n",
    "        if len(tokenized_text) > 512:\n",
    "            marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "            tokenized_text = self.tokenizer_.tokenize(marked_text)\n",
    "\n",
    "        indexed_tokens = self.tokenizer_.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "        tokens_tensor = tokens_tensor.to(device)\n",
    "        segments_tensors = segments_tensors.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model_(tokens_tensor, segments_tensors)\n",
    "            hidden_states = outputs[2]\n",
    "\n",
    "        token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "        token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "        token_vecs_sum = []\n",
    "        for token in token_embeddings:\n",
    "            sum_vec = torch.sum(token[-4:], dim=0)\n",
    "            token_vecs_sum.append(sum_vec)\n",
    "\n",
    "            token_vecs = hidden_states[-2][0]\n",
    "\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        return sentence_embedding.cpu().numpy()\n",
    "    \n",
    "    # Получение наиболее близкого текста к вопросу с использованием косинусного расстояния\n",
    "    def get_nearest_text(self, texts: List[str], question: str):\n",
    "        context_vectors = []\n",
    "        for paragraph in texts:\n",
    "            context_vectors.append(self.vectors_from_texts(paragraph))\n",
    "\n",
    "        question_vector = self.vectors_from_texts(question)\n",
    "        result = 1\n",
    "        result_id = 0\n",
    "        counter = 0\n",
    "        for vector in context_vectors:\n",
    "            if cosine(vector, question_vector) < result:\n",
    "                result = cosine(vector, question_vector)\n",
    "                result_id = counter\n",
    "            counter += 1\n",
    "\n",
    "        return texts[result_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b14ea9410449afabc64facc8843c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\emely\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c2340edcbd48bb94d3b1979e326c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9928dfa841d4579a71a05a65eddd4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c93a84569f493a9439df4628b7ec5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e20f420f78c44bf8400d254011de222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retriever = Retriever('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция-ридер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model_ = model\n",
    "        self.tokenizer_ = tokenizer\n",
    "\n",
    "    # Получение ответа на вопрос относительно заданного текста с использованием модели\n",
    "    def get_answer(self, text: str, question: str):\n",
    "        # Подготовка входных данных для модели с использованием токенизатора\n",
    "        encoding = self.tokenizer_.encode_plus(text=question, text_pair=text)\n",
    "        inputs = encoding['input_ids']\n",
    "        tokens = self.tokenizer_.convert_ids_to_tokens(inputs)\n",
    "        # Передача входных данных модели и получение выхода\n",
    "        output = self.model_(input_ids=torch.tensor([inputs]))\n",
    "        start_index = torch.argmax(output[0])\n",
    "        end_index = torch.argmax(output[1])\n",
    "\n",
    "        # Формирование ответа на основе токенов\n",
    "        answer = \" \".join(tokens[start_index:end_index+1])\n",
    "        answer = answer.replace(\" \", \"\")[1:].split(\"▁\")\n",
    "        answer = \" \".join(answer)\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca27141919d84c2f8ab5ba8027489625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/516 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae751161464b456c8810a0cb96b99e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/781 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc7fe8cbcb540c6ab68ad63e04b155d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1120ee3ada48a296dbb9e0bcebb3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef166c6377a14f6994088199a7ba83b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fda1ff5804499a8b48f97b765f470a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"AlexKay/xlm-roberta-large-qa-multilingual-finedtuned-ru\")\n",
    "reader = Reader(model, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция, внутри которой последовательно вызывается ретривер и ридер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    def __init__(self, retriever, reader):\n",
    "        self.retriever_ = retriever\n",
    "        self.reader_ = reader\n",
    "\n",
    "    def get_answer(self, texts: Union[str, List[str]], question: str):\n",
    "        if isinstance(texts, str):\n",
    "            answer = self.reader_.get_answer(texts, question)\n",
    "            return answer\n",
    "\n",
    "        sim_text = self.retriever_.get_nearest_text(texts, question)\n",
    "        answer = self.reader_.get_answer(sim_text, question)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_model = MyModel(retriever, reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Добавьте, пожалуйста, несколько примеров работы вашего кода, чтобы было видно, что вы передали тексты и вопрос, и получили правильный ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"sberquad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Answer is:       2050\n",
      "Predicted Answer is:  истекает договор аренды Байконура\n",
      "\n",
      "True Answer is:       9 млрд рублей в год\n",
      "Predicted Answer is:  \n",
      "\n",
      "True Answer is:       органические остатки\n",
      "Predicted Answer is:  органические остатки\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_qa(model, dataset, start_idx, end_idx, check_text_idx):\n",
    "    texts = dataset[\"context\"][start_idx:end_idx]\n",
    "    question = dataset[\"question\"][check_text_idx]\n",
    "\n",
    "    true_answer = dataset[\"answers\"][check_text_idx][\"text\"][0]\n",
    "    pred_answer = model.get_answer(texts, question)\n",
    "\n",
    "    print(f\"True Answer is:       {true_answer}\")\n",
    "    print(f\"Predicted Answer is:  {pred_answer}\")\n",
    "    print()\n",
    "\n",
    "texts = dataset[\"train\"]\n",
    "for i in [10, 11, 2]:\n",
    "    check_qa(qa_model, texts, 0, 10, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Всего четыре'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Сколько окенанов на Земле?\"\n",
    "texts = [\n",
    "    \"\"\"\n",
    "    Нил — одна из самых важных и древних рек в мире, играющая ключевую роль в истории и развитии региона. \n",
    "    Её истоки находятся в Восточной Африке, где сходятся две её основные притоки: Белый Нил и Голубой Нил. \n",
    "    Белый Нил начинается из озера Виктория, а Голубой Нил из озера Танганьика. Объединившись в Хартуме, столице Судана, они образуют Нил.\n",
    "    Нил протекает через несколько стран, включая Уганду, Кения, Эфиопию, Судан, и Египет. В Египте он впадает \n",
    "    в Средиземное море, создавая дельту Нила. Эта река была жизненно важной для древних цивилизаций, таких как древний \n",
    "    Египет, предоставляя воду для полива и сельского хозяйства.\n",
    "    Современные гидроэлектростанции, такие как Асуанская плотина в Египте, сделали Нил также важным источником энергии. \n",
    "    Однако регулирование потока реки также вызывает различные вопросы в области экологии и устойчивого развития.\n",
    "    Несмотря на свою длину и значимость, бассейн Нила также стал местом напряженности в связи с использованием его воды \n",
    "    между странами, протекающими вдоль его берегов.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Океаны Земли составляют около 99% общего объема жидкости на поверхности планеты. \n",
    "    Всего четыре океана - Тихий, Атлантический, Индийский и Северный Ледовитый.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Длинное слово в английском языке без использования каких-либо химических терминов - \"pneumonoultramicroscopicsilicovolcanoconiosis\". \n",
    "    Это название заболевания легких, вызванного вдыханием мельчайших кремниевых частиц.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "qa_model.get_answer(texts, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Нил'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Какая река была жизненно важной для древних цивилизаций?\"\n",
    "qa_model.get_answer(texts, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neumonoultramicroscopicsilicovolcanoconiosis\".'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Какое самое длиннок слово в английском языке?\"\n",
    "qa_model.get_answer(texts, question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
