{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymorphy2\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from gensim.models import Phrases\n",
    "# from skopt import BayesSearchCV\n",
    "# import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание на первую лабораторную: сделать классификацию тональности коротких текстов\n",
    "Датасет: https://github.com/sismetanin/rureviews \n",
    "Оставим только negative и positive отзывы\n",
    "- почистить текст, убрать знаки пунктуации (+токенизация)\n",
    "- лемматизировать текст \n",
    "- сгенерировать коллокации\n",
    "- использовать логистическую регрессию\n",
    "- поверх регрессии использовать GridSearchCV или BayesSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [02:42<00:00, 368.93it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"women-clothing-accessories.3-class.balanced.csv\", delimiter='\\t')\n",
    "# фильтруем создаем новый DataFrame, который содержит только те строки из df, \n",
    "df = df[df['sentiment'] != 'neautral']\n",
    "# убрали знаки препинания\n",
    "df['review_processed'] = df['review'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)).values\n",
    "# привели к нижнему регистру\n",
    "df['review_processed'] = df['review_processed'].apply(lambda x: x.lower())\n",
    "# разабъем слова\n",
    "df['review_processed'] = df['review_processed'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Создайте TF-IDF векторизатор (числовые признаки)\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "df['review_morphed'] = df['review_processed'].progress_apply(lambda x: [morph.parse(word)[0].normal_form for word in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "регресия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика -  0.9735222897024753\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X = tfidf_vectorizer.fit_transform([\" \".join(x) for x in df['review_morphed']])\n",
    "\n",
    "train_ds, test_ds, train_marks, test_marks = train_test_split(X, df['sentiment'], test_size=0.3, random_state=42)\n",
    "\n",
    "reg = LogisticRegression(max_iter=1000)\n",
    "reg.fit(train_ds, train_marks)\n",
    "predict = reg.predict_log_proba(test_ds)[:, 1]\n",
    "\n",
    "print('Метрика - ', roc_auc_score(test_marks, predict))\n",
    "\n",
    "# eli5.show_weights(estimator=reg, \n",
    "#                   feature_names= list(vectorizer.get_feature_names()),\n",
    "#                  top=(50, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search\n",
    "\n",
    "регрессия с использованием метода Grid Search для нахождения наилучших гиперпараметров модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Рос кривая -  0.9750521143633412\n",
      "{'C': 3.2, 'max_iter': 200}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [3, 3.2, 3.4, 3.6, 4],\n",
    "    'max_iter': [200, 400, 600, 1000]\n",
    "}\n",
    "# используем l2 регулизацию\n",
    "reg_gsCV = LogisticRegression(penalty='l2')\n",
    "gsCV = GridSearchCV(reg_gsCV, param_grid, verbose=True, n_jobs=8)\n",
    "gsCV.fit(train_ds, train_marks)\n",
    "predict = gsCV.predict_log_proba(test_ds)[:, 1]\n",
    "\n",
    "print(\"Рос кривая - \", roc_auc_score(test_marks, predict))\n",
    "print(gsCV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Он', 'поехал', 'в', 'Нью_Йорк']\n"
     ]
    }
   ],
   "source": [
    "# пример\n",
    "documents = [\"Нью Йорк большой город\", \"Нью Йорк\", \"Поеду в Нью Йорк на следующей неделе\"]\n",
    "sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "bigram = Phrases(sentence_stream, min_count=2, threshold=1)\n",
    "sent = ['Он', 'поехал', 'в', 'Нью', 'Йорк']\n",
    "print(bigram[sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "сгенерировать коллокации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Колокации - связи между словами рядом стоящими\n",
    "bigrams = Phrases(df['review_morphed'].values, min_count=2, threshold=1)\n",
    "df['review_processed_coll'] = df['review_morphed'].apply(lambda x: bigrams[x])\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X = tfidf_vectorizer.fit_transform([\" \".join(x) for x in df['review_processed_coll']])\n",
    "\n",
    "train_ds, test_ds, train_marks, test_marks = train_test_split(X, df['sentiment'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 73017)\n",
      "Рос кривая 0.9726146496239656\n"
     ]
    }
   ],
   "source": [
    "reg = LogisticRegression(max_iter=2000, n_jobs=-1)\n",
    "reg.fit(train_ds, train_marks)\n",
    "predict = reg.predict_log_proba(test_ds)[:, 1]\n",
    "\n",
    "print('Рос кривая - ', roc_auc_score(test_marks, predict))\n",
    "# eli5.show_weights(estimator=reg, \n",
    "#                   feature_names= list(vectorizer.get_feature_names()),\n",
    "#                  top=(50, 50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
