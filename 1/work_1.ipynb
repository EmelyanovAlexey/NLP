{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymorphy2\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание на первую лабораторную: сделать классификацию тональности коротких текстов\n",
    "Датасет: https://github.com/sismetanin/rureviews \n",
    "Оставим только negative и positive отзывы\n",
    "- почистить текст, убрать знаки пунктуации (+токенизация)\n",
    "- лемматизировать текст \n",
    "- сгенерировать коллокации\n",
    "- использовать логистическую регрессию\n",
    "- поверх регрессии использовать GridSearchCV или BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [02:44<00:00, 364.33it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"women-clothing-accessories.3-class.balanced.csv\", delimiter='\\t')\n",
    "df = df[df['sentiment'] != 'neautral']\n",
    "df['review_processed'] = df['review'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)).values\n",
    "df['review_processed'] = df['review_processed'].apply(lambda x: x.lower())\n",
    "df['review_processed'] = df['review_processed'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "df['review_morphed'] = df['review_processed'].progress_apply(lambda x: [morph.parse(word)[0].normal_form for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 51549)\n",
      "0.9735222897024753\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X = vectorizer.fit_transform([\" \".join(x) for x in df['review_morphed']])\n",
    "print(X.shape)\n",
    "\n",
    "train_ds, test_ds, train_marks, test_marks = train_test_split(X, df['sentiment'], test_size=0.3, random_state=42)\n",
    "\n",
    "reg = LogisticRegression(max_iter=1000)\n",
    "reg.fit(train_ds, train_marks)\n",
    "predict = reg.predict_log_proba(test_ds)[:, 1]\n",
    "\n",
    "print(roc_auc_score(test_marks, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n",
      "    call_item = call_queue.get(block=True, timeout=timeout)\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "ModuleNotFoundError: No module named 'sklearn.utils.parallel'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\emely\\AppData\\Local\\Temp\\ipykernel_19968\\3350847397.py\", line 8, in <module>\n",
      "    gsCV.fit(train_ds, train_marks)\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 898, in fit\n",
      "    self.best_index_\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 1422, in _run_search\n",
      "    then a parameter is sampled using that dict as above.\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 845, in evaluate_candidates\n",
      "    elif len(out) != n_candidates * n_splits:\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1952, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1595, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1699, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 1734, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 736, in get_result\n",
      "    return self._return_or_raise()\n",
      "  File \"c:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 754, in _return_or_raise\n",
      "    raise self._result\n",
      "joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\emely\\AppData\\Roaming\\Python\\Python310\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [3.5, 3.2, 3.1, 3, 2.9],\n",
    "    'max_iter': [150, 250, 500, 1000, 1500]\n",
    "}\n",
    "\n",
    "reg_gsCV = LogisticRegression(penalty='l2')\n",
    "gsCV = GridSearchCV(reg_gsCV, param_grid, verbose=True, n_jobs=8)\n",
    "gsCV.fit(train_ds, train_marks)\n",
    "predict = gsCV.predict_log_proba(test_ds)[:, 1]\n",
    "\n",
    "print(\"ROC:\", roc_auc_score(test_marks, predict))\n",
    "print(gsCV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"women-clothing-accessories.3-class.balanced.csv\", encoding=\"utf8\", delimiter='\\t')\n",
    "# # фильтруем создаем новый DataFrame, который содержит только те строки из df, \n",
    "# # где значение в столбце 'sentiment' не равно 'neautral'.\n",
    "# df = df[df['sentiment'] != 'neautral']\n",
    "# # убрали знаки препинания\n",
    "# df['review_processed'] = df['review'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x)).values\n",
    "# # привели к нижнему регистру\n",
    "# df['review_processed'] = df['review_processed'].apply(lambda x: x.lower())\n",
    "# # разабъем слова\n",
    "# df['review_processed'] = df['review_processed'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# # вернет информацию о его форме, грамматических характеристиках и т. д, лемантизируем\n",
    "# morph = pymorphy2.MorphAnalyzer()\n",
    "# df['review_morphed'] = df['review_processed'].progress_apply(lambda x: [morph.parse(word)[0].normal_form for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Создайте TF-IDF векторизатор (числовые признаки)\n",
    "# tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
    "# X = tfidf_vectorizer.fit_transform([\" \".join(x) for x in df['review_morphed']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, train_marks, y_test = train_test_split(X, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# reg = LogisticRegression(max_iter=1000)\n",
    "# reg.fit(X_train, train_marks)\n",
    "# predict = reg.predict_log_proba(X_test)[:, 1]\n",
    "\n",
    "# print('Метрика - ', roc_auc_score(y_test, predict))\n",
    "\n",
    "# # eli5.show_weights(estimator=reg, \n",
    "# #                   feature_names= list(tfidf_vectorizer.get_feature_names()),\n",
    "# #                  top=(50, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds, test_ds, train_marks, test_marks = train_test_split(X, df['sentiment'], test_size=0.3, random_state=42)\n",
    "# param_grid = {\n",
    "#     'C': [3.5, 3.2, 3.1, 3, 2.9],\n",
    "#     'max_iter': [150, 250, 500, 1000, 1500]\n",
    "# }\n",
    "\n",
    "# reg_gsCV = LogisticRegression(penalty='l2')\n",
    "# gsCV = GridSearchCV(reg_gsCV, param_grid, verbose=True, n_jobs=8)\n",
    "# gsCV.fit(train_ds, train_marks)\n",
    "# predict = gsCV.predict_log_proba(test_ds)[:, 1]\n",
    "\n",
    "# print(\"ROC:\", roc_auc_score(test_marks, predict))\n",
    "# print(gsCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
